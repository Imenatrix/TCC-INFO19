{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imenatrix/TCC-INFO19/blob/feature-dqfd/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "jQ0equmGYKbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo add-apt-repository -y ppa:openjdk-r/ppa\n",
        "!sudo apt-get purge openjdk-*\n",
        "!sudo apt-get install openjdk-8-jdk"
      ],
      "metadata": {
        "id": "YRB8g4TMYMC_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip3 install minerl"
      ],
      "metadata": {
        "id": "kDkezv40YO5h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "-zuJdSSzX9yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from itertools import product\n",
        "from collections import OrderedDict\n",
        "from google.cloud import storage\n",
        "from google.colab import drive\n",
        "\n",
        "import minerl\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from keras import Model\n",
        "from keras.layers import *\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "FLc3fue_X9TE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe433208-057f-4dc8-9137-dd5a1fea5c46"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "    strategy = tf.distribute.get_strategy()"
      ],
      "metadata": {
        "id": "XJ4KnwlO2VA8",
        "outputId": "23b990da-d7a6-459a-dc36-a02e76ddf7ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtFFwJOuYGUe"
      },
      "source": [
        "# Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrappers = {}\n",
        "\n",
        "def register_wrapper(name, wrapper):\n",
        "    wrappers[name] = wrapper"
      ],
      "metadata": {
        "id": "MVMjGEEkgSl7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTI-JMK6YGUf"
      },
      "source": [
        "## Amiranas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ANVPcDTdYGUf"
      },
      "outputs": [],
      "source": [
        "class ActionManager:\n",
        "    \"\"\"Main minecraft action wrapper. Simplifies action space to 130 discrete actions\"\"\"\n",
        "\n",
        "    def __init__(self, c_action_magnitude=22.5):\n",
        "        self.c_action_magnitude = c_action_magnitude\n",
        "\n",
        "        self.zero_action = OrderedDict([('attack', 0),\n",
        "                                        ('back', 0),\n",
        "                                        ('camera', np.array([0., 0.])),\n",
        "                                        ('forward', 0),\n",
        "                                        ('jump', 0),\n",
        "                                        ('left', 0),\n",
        "                                        ('right', 0),\n",
        "                                        ('sneak', 0),\n",
        "                                        ('sprint', 0)])\n",
        "\n",
        "        # camera discretization:\n",
        "        self.camera_dict = OrderedDict([\n",
        "            ('turn_up', np.array([-c_action_magnitude, 0.])),\n",
        "            ('turn_down', np.array([c_action_magnitude, 0.])),\n",
        "            ('turn_left', np.array([0., -c_action_magnitude])),\n",
        "            ('turn_right', np.array([0., c_action_magnitude]))\n",
        "        ])\n",
        "\n",
        "        self.fully_connected_no_camera = ['attack', 'back', 'forward', 'jump', 'left', 'right', 'sprint']\n",
        "        self.camera_actions = ['turn_up', 'turn_down', 'turn_left', 'turn_right']\n",
        "        self.fully_connected = self.fully_connected_no_camera + self.camera_actions\n",
        "\n",
        "        # following action combinations are excluded:\n",
        "        self.exclude = [('forward', 'back'), ('left', 'right'), ('attack', 'jump'),\n",
        "                        ('turn_up', 'turn_down', 'turn_left', 'turn_right')]\n",
        "\n",
        "        # sprint only allowed when forward is used:\n",
        "        self.only_if = [('sprint', 'forward')]\n",
        "\n",
        "        # Maximal allowed mount of actions within one action:\n",
        "        self.remove_size = 3\n",
        "\n",
        "        # if more than 3 actions are present, actions are removed using this list until only 3 actions remain:\n",
        "        self.remove_first_list = ['sprint', 'left', 'right', 'back',\n",
        "                                  'turn_up', 'turn_down', 'turn_left', 'turn_right',\n",
        "                                  'attack', 'jump', 'forward']\n",
        "\n",
        "        self.fully_connected_list = list(product(range(2), repeat=len(self.fully_connected)))\n",
        "\n",
        "        remove = []\n",
        "        for el in self.fully_connected_list:\n",
        "            for tuple_ in self.exclude:\n",
        "                if sum([el[self.fully_connected.index(a)] for a in tuple_]) > 1:\n",
        "                    if el not in remove:\n",
        "                        remove.append(el)\n",
        "            for a, b in self.only_if:\n",
        "                if el[self.fully_connected.index(a)] == 1 and el[self.fully_connected.index(b)] == 0:\n",
        "                    if el not in remove:\n",
        "                        remove.append(el)\n",
        "            if sum(el) > self.remove_size:\n",
        "                if el not in remove:\n",
        "                    remove.append(el)\n",
        "\n",
        "        for r in remove:\n",
        "            self.fully_connected_list.remove(r)\n",
        "\n",
        "        self.action_list = []\n",
        "        for el in self.fully_connected_list:\n",
        "            new_action = copy.deepcopy(self.zero_action)\n",
        "            for key, value in zip(self.fully_connected, el):\n",
        "                if key in self.camera_actions:\n",
        "                    if value:\n",
        "                        new_action['camera'] = self.camera_dict[key]\n",
        "                else:\n",
        "                    new_action[key] = value\n",
        "            self.action_list.append(new_action)\n",
        "\n",
        "        self.num_action_ids_list = [len(self.action_list)]\n",
        "        self.act_continuous_size = 0\n",
        "\n",
        "    def get_action(self, id_):\n",
        "        a = copy.deepcopy(self.action_list[int(id_)])\n",
        "        a['camera'] += np.random.normal(0., 0.5, 2)\n",
        "        return a\n",
        "\n",
        "    def print_action(self, id_):\n",
        "        a = copy.deepcopy(self.action_list[int(id_)])\n",
        "        out = \"\"\n",
        "        for k, v in a.items():\n",
        "            if k != 'camera':\n",
        "                if v != 0:\n",
        "                    out += f'{k} '\n",
        "            else:\n",
        "                if (v != np.zeros(2)).any():\n",
        "                    out += k\n",
        "\n",
        "        print(out)\n",
        "\n",
        "    def get_id(self, action, batch_size):\n",
        "\n",
        "        coiso = np.zeros((batch_size,), dtype=int)\n",
        "        action = copy.deepcopy(action)\n",
        "        for i in range(batch_size):\n",
        "\n",
        "            # discretize 'camera':\n",
        "            camera = action['camera'][i]\n",
        "            camera_action_amount = 0\n",
        "            if - self.c_action_magnitude / 2. < camera[0] < self.c_action_magnitude / 2.:\n",
        "                action['camera'][i][0] = 0.\n",
        "                if - self.c_action_magnitude / 2. < camera[1] < self.c_action_magnitude / 2.:\n",
        "                    action['camera'][i][1] = 0.\n",
        "                else:\n",
        "                    camera_action_amount = 1\n",
        "                    action['camera'][i][1] = self.c_action_magnitude * np.sign(camera[1])\n",
        "            else:\n",
        "                camera_action_amount = 1\n",
        "                action['camera'][i][0] = self.c_action_magnitude * np.sign(camera[0])\n",
        "\n",
        "                action['camera'][i][1] = 0.\n",
        "\n",
        "            # simplify action:\n",
        "            for tuple_ in self.exclude:\n",
        "                if len(tuple_) == 2:\n",
        "                    a, b = tuple_\n",
        "                    if action[a][i] and action[b][i]:\n",
        "                        action[b][i] = 0\n",
        "            for a, b in self.only_if:\n",
        "                if not action[b][i]:\n",
        "                    if action[a][i]:\n",
        "                        action[a][i] = 0\n",
        "            for a in self.remove_first_list:\n",
        "                if sum([action[key][i] for key in self.fully_connected_no_camera]) > \\\n",
        "                        (self.remove_size - camera_action_amount):\n",
        "                    if a in self.camera_actions:\n",
        "                        action['camera'][i] = np.array([0., 0.])\n",
        "                        camera_action_amount = 0\n",
        "                    else:\n",
        "                        action[a][i] = 0\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            # set one_hot camera keys:\n",
        "            for key in self.camera_actions:\n",
        "                action[key] = [0 for x in range(batch_size)]\n",
        "            for key, val in self.camera_dict.items():\n",
        "                if (action['camera'][i] == val).all():\n",
        "                    action[key][i] = 1\n",
        "                    break\n",
        "\n",
        "            non_separate_values = tuple(action[key][i] for key in self.fully_connected)\n",
        "\n",
        "            coiso[i] = self.fully_connected_list.index(non_separate_values)\n",
        "        return coiso\n",
        "\n",
        "    def get_left_right_reversed_mapping(self):\n",
        "        action_mapping = []\n",
        "        for action in self.action_list:\n",
        "            reversed_action = copy.deepcopy(action)\n",
        "            if action['left'] == 1:\n",
        "                reversed_action['left'] = 0\n",
        "                reversed_action['right'] = 1\n",
        "                assert action['right'] == 0\n",
        "            if action['right'] == 1:\n",
        "                reversed_action['right'] = 0\n",
        "                reversed_action['left'] = 1\n",
        "                assert action['left'] == 0\n",
        "            if (action['camera'] == [0, -22.5]).all():\n",
        "                reversed_action['camera'][1] = 22.5\n",
        "            if (action['camera'] == [0, 22.5]).all():\n",
        "                reversed_action['camera'][1] = -22.5\n",
        "\n",
        "            rev_action_id = self.get_id(reversed_action)\n",
        "            action_mapping.append(rev_action_id)\n",
        "\n",
        "        return action_mapping\n",
        "\n",
        "manager = ActionManager()\n",
        "register_wrapper('amiranas', manager.get_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW5tdiaUYGUi"
      },
      "source": [
        "## Baseline Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8dNacU-hYGUl"
      },
      "outputs": [],
      "source": [
        "def dataset_action_batch_to_actions(dataset_actions, batch_size, camera_margin=3):\n",
        "    \"\"\"\n",
        "    Turn a batch of actions from dataset (`batch_iter`) to a numpy\n",
        "    array that corresponds to batch of actions of ActionShaping wrapper (_actions).\n",
        "\n",
        "    Camera margin sets the threshold what is considered \"moving camera\".\n",
        "\n",
        "    Note: Hardcoded to work for actions in ActionShaping._actions, with \"intuitive\"\n",
        "        ordering of actions.\n",
        "        If you change ActionShaping._actions, remember to change this!\n",
        "\n",
        "    Array elements are integers corresponding to actions, or \"-1\"\n",
        "    for actions that did not have any corresponding discrete match.\n",
        "    \"\"\"\n",
        "    # There are dummy dimensions of shape one\n",
        "    camera_actions = dataset_actions[\"camera\"].squeeze()\n",
        "    attack_actions = dataset_actions[\"attack\"].squeeze()\n",
        "    forward_actions = dataset_actions[\"forward\"].squeeze()\n",
        "    jump_actions = dataset_actions[\"jump\"].squeeze()\n",
        "    actions = np.zeros((batch_size,), dtype=int)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Moving camera is most important (horizontal first)\n",
        "        if camera_actions[i][0] < -camera_margin:\n",
        "            actions[i] = 4\n",
        "        elif camera_actions[i][0] > camera_margin:\n",
        "            actions[i] = 5\n",
        "        elif camera_actions[i][1] > camera_margin:\n",
        "            actions[i] = 6\n",
        "        elif camera_actions[i][1] < -camera_margin:\n",
        "            actions[i] = 7\n",
        "        elif forward_actions[i] == 1:\n",
        "            if jump_actions[i] == 1:\n",
        "                actions[i] = 3\n",
        "            else:\n",
        "                actions[i] = 2\n",
        "        elif attack_actions[i] == 1:\n",
        "            actions[i] = 1\n",
        "        else:\n",
        "            # No reasonable mapping (would be no-op)\n",
        "            actions[i] = 0\n",
        "    return actions\n",
        "\n",
        "register_wrapper('baseline_notebook', dataset_action_batch_to_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "WhuJkgDYJLmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env MINERL_DATA_ROOT=/home/minerl\n",
        "%env GOOGLE_APPLICATION_CREDENTIALS=/content/drive/MyDrive/key.json"
      ],
      "metadata": {
        "id": "zwnzpW4IYRa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a8e953-6ba5-4cd1-e389-a3597eeeedad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: MINERL_DATA_ROOT=/home/minerl\n",
            "env: GOOGLE_APPLICATION_CREDENTIALS=/content/drive/MyDrive/key.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m minerl.data.download --environment \"MineRLTreechop-v0\""
      ],
      "metadata": {
        "id": "GrbLl57EYS3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcafadd6-c44e-403f-be1c-a891557e60d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl.data.download' found in sys.modules after import of package 'minerl.data', but prior to execution of 'minerl.data.download'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "\u001b[32m2022-11-23 11:21:42\u001b[0m \u001b[35mfa1951358374\u001b[0m \u001b[34m__main__[3776]\u001b[0m \u001b[1;30mINFO\u001b[0m Downloading dataset for MineRLTreechop-v0 to /home/minerl\n",
            "\u001b[32m2022-11-23 11:21:42\u001b[0m \u001b[35mfa1951358374\u001b[0m \u001b[34m__main__[3776]\u001b[0m \u001b[1;30mINFO\u001b[0m Starting download ...\n",
            "\u001b[32m2022-11-23 11:21:42\u001b[0m \u001b[35mfa1951358374\u001b[0m \u001b[34mroot[3776]\u001b[0m \u001b[1;30mDEBUG\u001b[0m \u001b[32mChoosing mirror ...\u001b[0m\n",
            "\u001b[32m2022-11-23 11:21:43\u001b[0m \u001b[35mfa1951358374\u001b[0m \u001b[34mroot[3776]\u001b[0m \u001b[1;30mDEBUG\u001b[0m \u001b[32mPicked https://minerl.s3.amazonaws.com/v4/MineRLTreechop-v0.tar ping=222.986ms\u001b[0m\n",
            "\u001b[32m2022-11-23 11:21:43\u001b[0m \u001b[35mfa1951358374\u001b[0m \u001b[34mroot[3776]\u001b[0m \u001b[1;30mDEBUG\u001b[0m \u001b[32mStarting download at 0.0MB\u001b[0m\n",
            "\u001b[32m2022-11-23 11:21:43\u001b[0m \u001b[35mfa1951358374\u001b[0m \u001b[34mroot[3776]\u001b[0m \u001b[1;30mDEBUG\u001b[0m \u001b[32mFile size is 1510.7MB\u001b[0m\n",
            "Download: https://minerl.s3.amazonaws.com/v4/MineRLTreechop-v0.tar: 100% 1511.0/1510.73792 [00:28<00:00, 52.80MB/s]\n",
            "\u001b[32m2022-11-23 11:22:12\u001b[0m \u001b[35mfa1951358374\u001b[0m \u001b[34mroot[3776]\u001b[0m \u001b[1;30mINFO\u001b[0m Success - downloaded /home/minerl/download/v4/MineRLTreechop-v0.tar\n",
            "\u001b[32m2022-11-23 11:22:12\u001b[0m \u001b[35mfa1951358374\u001b[0m \u001b[34mroot[3776]\u001b[0m \u001b[1;30mINFO\u001b[0m Extracting downloaded files - this may take some time\n",
            "\u001b[32m2022-11-23 11:22:19\u001b[0m \u001b[35mfa1951358374\u001b[0m \u001b[34mroot[3776]\u001b[0m \u001b[1;30mINFO\u001b[0m Success - extracted files to /home/minerl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "    # The ID of your GCS bucket\n",
        "    # bucket_name = \"your-bucket-name\"\n",
        "    # The path to your file to upload\n",
        "    # source_file_name = \"local/path/to/file\"\n",
        "    # The ID of your GCS object\n",
        "    # destination_blob_name = \"storage-object-name\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    \n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n"
      ],
      "metadata": {
        "id": "O-eBZe1Ofo-z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bytes_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def float_feature(value):\n",
        "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def int64_feature(value):\n",
        "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "def int64_list_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
      ],
      "metadata": {
        "id": "MoCWq5GvJRmV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import minerl\n",
        "import os\n",
        "import time\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "from minerl.data.util import multimap\n",
        "import random\n",
        "\n",
        "MINERL_DATA_ROOT = os.getenv('MINERL_DATA_ROOT')\n",
        "\n",
        "\n",
        "\n",
        "def stack(*args):\n",
        "    return np.stack(args)\n",
        "\n",
        "\n",
        "class BufferedBatchIter:\n",
        "    all_trajectories = None\n",
        "    \"\"\"\n",
        "    A class that maintains and exposes an iterator which loads trajectories into a\n",
        "    configurably-sized buffer, samples batches from that buffer, and refills the buffer\n",
        "    when necessary.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 data_pipeline,\n",
        "                 buffer_target_size=50000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_pipeline: A data pipeline object that you want to construct an iterator from\n",
        "            buffer_target_size: How large you'd like your data buffer to be (in units of timesteps)\n",
        "\n",
        "            Note that this is not an exact cap, since we don't know how large a trajectory will be\n",
        "            until we load it in. This implementation tries to maintain a buffer size by keeping\n",
        "            track of the average size of trajectories in this data pipeline, and loading a new\n",
        "            trajectory when the size of the buffer is more than <average_size> below the target\n",
        "        \"\"\"\n",
        "        self.data_pipeline = data_pipeline\n",
        "        self.data_buffer = []\n",
        "        self.buffer_target_size = buffer_target_size\n",
        "        self.traj_sizes = []\n",
        "        self.avg_traj_size = 0\n",
        "        if BufferedBatchIter.all_trajectories == None:\n",
        "            self.all_trajectories = self.data_pipeline.get_trajectory_names()\n",
        "            BufferedBatchIter.all_trajectories = deepcopy(self.all_trajectories)\n",
        "        else:\n",
        "            self.all_trajectories = BufferedBatchIter.all_trajectories\n",
        "        # available_trajectories is a dynamic, per-epoch list that will keep track of\n",
        "        # which trajectories we haven't yet used in a given epoch\n",
        "        self.available_trajectories = deepcopy(self.all_trajectories)\n",
        "        #random.shuffle(self.available_trajectories)\n",
        "\n",
        "    def optionally_fill_buffer(self):\n",
        "        \"\"\"\n",
        "        This method is run after every batch, but only actually executes a buffer\n",
        "        refill and re-shuffle if more data is needed\n",
        "        \"\"\"\n",
        "        buffer_updated = False\n",
        "\n",
        "        # Add trajectories to the buffer if the remaining space is\n",
        "        # greater than our anticipated trajectory size (in the form of the empirical average)\n",
        "        while (self.buffer_target_size - len(self.data_buffer)) > self.avg_traj_size:\n",
        "            if len(self.available_trajectories) == 0:\n",
        "                return\n",
        "            traj_to_load = self.available_trajectories.pop()\n",
        "            data_loader = self.data_pipeline.load_data(traj_to_load)\n",
        "            traj_len = 0\n",
        "            for data_tuple in data_loader:\n",
        "                traj_len += 1\n",
        "                self.data_buffer.append(data_tuple)\n",
        "\n",
        "            self.traj_sizes.append(traj_len)\n",
        "            self.avg_traj_size = np.mean(self.traj_sizes)\n",
        "            buffer_updated = True\n",
        "        #if buffer_updated:\n",
        "            #random.shuffle(self.data_buffer)\n",
        "\n",
        "    def get_batch(self, batch_size):\n",
        "        \"\"\"A simple utility method for constructing a return batch in the expected format\"\"\"\n",
        "        ret_dict_list = []\n",
        "        for _ in range(batch_size):\n",
        "            data_tuple = self.data_buffer.pop()\n",
        "            ret_dict = dict(obs=data_tuple[0],\n",
        "                            act=data_tuple[1],\n",
        "                            reward=data_tuple[2],\n",
        "                            next_obs=data_tuple[3],\n",
        "                            done=data_tuple[4])\n",
        "            ret_dict_list.append(ret_dict)\n",
        "        return multimap(stack, *ret_dict_list)\n",
        "\n",
        "    def buffered_batch_iter(self, batch_size, num_epochs=None, num_batches=None):\n",
        "        \"\"\"\n",
        "        The actual generator method that returns batches. You can specify either\n",
        "        a desired number of batches, or a desired number of epochs, but not both,\n",
        "        since they might conflict.\n",
        "\n",
        "        ** You must specify one or the other **\n",
        "\n",
        "        Args:\n",
        "            batch_size: The number of transitions/timesteps to be returned in each batch\n",
        "            num_epochs: Optional, how many full passes through all trajectories to return\n",
        "            num_batches: Optional, how many batches to return\n",
        "\n",
        "        \"\"\"\n",
        "        assert num_batches is not None or num_epochs is not None, \"One of num_epochs or \" \\\n",
        "                                                                  \"num_batches must be non-None\"\n",
        "        assert num_batches is None or num_epochs is None, \"You cannot specify both \" \\\n",
        "                                                          \"num_batches and num_epochs\"\n",
        "\n",
        "        epoch_count = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        while True:\n",
        "            # If we've hit the desired number of epochs\n",
        "            if num_epochs is not None and epoch_count >= num_epochs:\n",
        "                return\n",
        "            # If we've hit the desired number of batches\n",
        "            if num_batches is not None and batch_count >= num_batches:\n",
        "                return\n",
        "            # Refill the buffer if we need to\n",
        "            # (doing this before getting batch so it'll run on the first iteration)\n",
        "            self.optionally_fill_buffer()\n",
        "            ret_batch = self.get_batch(batch_size=batch_size)\n",
        "            batch_count += 1\n",
        "            if len(self.data_buffer) < batch_size:\n",
        "                assert len(self.available_trajectories) == 0, \"You've reached the end of your \" \\\n",
        "                                                              \"data buffer while still having \" \\\n",
        "                                                              \"trajectories available; \" \\\n",
        "                                                              \"something seems to have gone wrong\"\n",
        "                epoch_count += 1\n",
        "                self.available_trajectories = deepcopy(self.all_trajectories)\n",
        "                #random.shuffle(self.available_trajectories)\n",
        "\n",
        "            keys = ('obs', 'act', 'reward', 'next_obs', 'done')\n",
        "            yield tuple([ret_batch[key] for key in keys])\n"
      ],
      "metadata": {
        "id": "2PEKJZjpjAO6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_expert_data(wrapper, examples_per_file, dataset_dir):\n",
        "    wrap = wrappers[wrapper]\n",
        "\n",
        "    data = minerl.data.make('MineRLTreechop-v0')\n",
        "    iterator = BufferedBatchIter(data, 30000)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(dataset_dir)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    state_buffer = []\n",
        "    action_buffer = []\n",
        "    reward_buffer = []\n",
        "    state_next_buffer = []\n",
        "    done_buffer = []\n",
        "\n",
        "    gamma = 0.99\n",
        "\n",
        "    filename = f'{i}.tfrecord'\n",
        "    filepath = f'{dataset_dir}/{filename}'\n",
        "    blobpath = f'tfrecords_nstep/{filename}'\n",
        "    writer = tf.io.TFRecordWriter(filepath)\n",
        "\n",
        "    examples = 0\n",
        "\n",
        "    for state, action, reward, state_next, done in iterator.buffered_batch_iter(examples_per_file, num_epochs=1):\n",
        "        state = state['pov'].squeeze().astype(np.float32) / 255\n",
        "        state_next = state_next['pov'].squeeze().astype(np.float32) / 255\n",
        "        action = wrap(action, examples_per_file).squeeze()\n",
        "\n",
        "        for nstep_state, nstep_action, nstep_reward, nstep_state_next, nstep_done in zip(state, action, reward, state_next, done):\n",
        "            state_buffer.append(nstep_state)\n",
        "            action_buffer.append(nstep_action)\n",
        "            reward_buffer.append(nstep_reward)\n",
        "            state_next_buffer.append(nstep_state_next)\n",
        "            done_buffer.append(nstep_done)\n",
        "\n",
        "            if len(reward_buffer) == 10:\n",
        "\n",
        "                nstep_reward_sum = 0\n",
        "                episode_ended = nstep_done\n",
        "                do_while = True\n",
        "\n",
        "                while episode_ended or do_while:\n",
        "                    for j in range(len(reward_buffer)):\n",
        "                        nstep_reward_sum += gamma ** j * reward_buffer[j]\n",
        "\n",
        "                    filename = f'{i}.tfrecord'\n",
        "                    filepath = f'{dataset_dir}/{filename}'\n",
        "                    blobpath = f'tfrecords_nstep/{filename}'\n",
        "\n",
        "                    example = encode_example(\n",
        "                        state_buffer[0],\n",
        "                        action_buffer[0],\n",
        "                        reward_buffer[0],\n",
        "                        state_next_buffer[0],\n",
        "                        done_buffer[0],\n",
        "                        len(reward_buffer),\n",
        "                        nstep_reward_sum,\n",
        "                        nstep_state_next,\n",
        "                        episode_ended\n",
        "                    )\n",
        "                    writer.write(example.SerializeToString())\n",
        "                    examples += 1\n",
        "\n",
        "                    if examples == examples_per_file:\n",
        "\n",
        "                        writer.close()\n",
        "\n",
        "                        upload_blob('minerl_data_records', filepath, blobpath)\n",
        "                        os.remove(filepath)\n",
        "\n",
        "                        examples = 0\n",
        "\n",
        "                        i += 1\n",
        "\n",
        "                        filename = f'{i}.tfrecord'\n",
        "                        filepath = f'{dataset_dir}/{filename}'\n",
        "                        blobpath = f'tfrecords_nstep/{filename}'\n",
        "                        writer = tf.io.TFRecordWriter(filepath)\n",
        "\n",
        "                    if done_buffer[0]:\n",
        "                        episode_ended = False\n",
        "\n",
        "                    state_buffer = state_buffer[1:]\n",
        "                    action_buffer = action_buffer[1:]\n",
        "                    reward_buffer = reward_buffer[1:]\n",
        "                    state_next_buffer = state_next_buffer[1:]\n",
        "                    done_buffer = done_buffer[1:]\n",
        "\n",
        "                    do_while = False\n"
      ],
      "metadata": {
        "id": "bFqbXyB8M6jh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def listdir(dir):\n",
        "    return list(map(\n",
        "        lambda file: dir + '/' + file,\n",
        "        os.listdir(dir)\n",
        "    ))"
      ],
      "metadata": {
        "id": "ROT5pG0rSL9M"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(filenames, batch_size):\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "    ignore_order = tf.data.Options()\n",
        "    ignore_order.deterministic = False  # disable order, increase speed\n",
        "\n",
        "    return (\n",
        "        tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n",
        "            .with_options(ignore_order)\n",
        "            .map(decode_example, num_parallel_calls=AUTOTUNE)\n",
        "            .repeat()\n",
        "            .batch(batch_size)\n",
        "            .prefetch(AUTOTUNE)\n",
        "    )\n",
        "\n",
        "def create_val_dataset(filenames, batch_size):\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "    ignore_order = tf.data.Options()\n",
        "    ignore_order.deterministic = False  # disable order, increase speed\n",
        "\n",
        "    return (\n",
        "        tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n",
        "            .with_options(ignore_order)\n",
        "            .map(decode_example, num_parallel_calls=AUTOTUNE)\n",
        "            .repeat()\n",
        "            .batch(batch_size)\n",
        "            .prefetch(AUTOTUNE)\n",
        "    )"
      ],
      "metadata": {
        "id": "Kv80oX7LSMgQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_example(state, action, reward, state_next, done, nsteps, nstep_reward_sum, nstep_state_next, nstep_done):\n",
        "  feature = {\n",
        "      'state' : bytes_feature(tf.io.serialize_tensor(state)),\n",
        "      'action' : int64_feature(action),\n",
        "      'reward' : float_feature(reward),\n",
        "      'state_next' : bytes_feature(tf.io.serialize_tensor(state_next)),\n",
        "      'done' : int64_feature(done),\n",
        "      'nsteps' : int64_feature(nsteps),\n",
        "      'nstep_reward_sum' : float_feature(nstep_reward_sum),\n",
        "      'nstep_state_next' : bytes_feature(tf.io.serialize_tensor(nstep_state_next)),\n",
        "      'nstep_done' : int64_feature(nstep_done)\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "\n",
        "def decode_example(example):\n",
        "  feature_description = {\n",
        "    'state': tf.io.FixedLenFeature([], tf.string),\n",
        "    'action': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'reward' : tf.io.FixedLenFeature([], tf.float32),\n",
        "    'state_next' : tf.io.FixedLenFeature([], tf.string),\n",
        "    'done' : tf.io.FixedLenFeature([], tf.int64),\n",
        "    'nsteps': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'nstep_reward_sum' : tf.io.FixedLenFeature([], tf.float32),\n",
        "    'nstep_state_next' : tf.io.FixedLenFeature([], tf.string),\n",
        "    'nstep_done' : tf.io.FixedLenFeature([], tf.int64)\n",
        "  }\n",
        "  example = tf.io.parse_single_example(example, feature_description)\n",
        "\n",
        "  state = tf.io.parse_tensor(example['state'], out_type=tf.float32)\n",
        "  state = tf.reshape(state, (64, 64, 3))\n",
        "\n",
        "  state_next = tf.io.parse_tensor(example['state_next'], out_type=tf.float32)\n",
        "  state_next = tf.reshape(state_next, (64, 64, 3))\n",
        "\n",
        "  nstep_state_next = tf.io.parse_tensor(example['nstep_state_next'], out_type=tf.float32)\n",
        "  nstep_state_next = tf.reshape(nstep_state_next, (64, 64, 3))\n",
        "\n",
        "  return state, example['action'], example['reward'], state_next, example['done'], example['nsteps'], example['nstep_reward_sum'], nstep_state_next, example['nstep_done']"
      ],
      "metadata": {
        "id": "jRKdNnHjRBjU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDfAkWunYGUZ"
      },
      "source": [
        "# Trainers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainers = {}\n",
        "\n",
        "def register_trainer(name, trainer):\n",
        "    trainers[name] = trainer"
      ],
      "metadata": {
        "id": "2yys3vf-nLA9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQFD"
      ],
      "metadata": {
        "id": "A2piAVS4fTFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from collections import namedtuple\n",
        "\n",
        "def dqfd(model, model_target, loss_fn, optimizer, dataset, val_dataset, steps, val_steps, epochs):\n",
        "    dataset = iter(dataset)\n",
        "    val_dataset = iter(val_dataset)\n",
        "\n",
        "    def dqfd_metrics(inputs):\n",
        "\n",
        "        (state, action, reward, state_next, done, nsteps, nstep_reward_sum, nstep_state_next, nstep_done) = inputs\n",
        "\n",
        "        target_q_values = model_target(state)\n",
        "        future_rewards = model_target(state_next)\n",
        "        nstep_future_rewards = model_target(nstep_state_next)\n",
        "\n",
        "        max_future_reward =  tf.reduce_max(future_rewards, axis=1)\n",
        "        max_nstep_future_reward = tf.reduce_max(nstep_future_rewards, axis=1)\n",
        "\n",
        "        updated_q_values = reward + gamma * max_future_reward\n",
        "        updated_q_values = updated_q_values * tf.cast(1 - done, dtype=tf.float32) - tf.cast(done, dtype=tf.float32)\n",
        "\n",
        "        updated_nstep_q_values = nstep_reward_sum + gamma ** tf.cast(nsteps, dtype=tf.float32) + max_nstep_future_reward\n",
        "        updated_nstep_q_values = updated_nstep_q_values * tf.cast(1 - nstep_done, dtype=tf.float32) - tf.cast(done, dtype=tf.float32)\n",
        "\n",
        "        masks = tf.one_hot(action, 112)\n",
        "\n",
        "        return (target_q_values, updated_q_values, updated_nstep_q_values, masks)\n",
        "\n",
        "    def val_step(state):\n",
        "        (state, action, reward, state_next, done, nsteps, nstep_reward_sum, nstep_state_next, nstep_done) = inputs\n",
        "        (target_q_values, updated_q_values, updated_nstep_q_values, masks) = dqfd_metrics(inputs)\n",
        "\n",
        "        q_values = model(state)\n",
        "        q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "        lmc_loss = tf.reduce_mean(\n",
        "            tf.reduce_max(\n",
        "                tf.cast(\n",
        "                    tf.equal(\n",
        "                        masks,\n",
        "                        0\n",
        "                    ),\n",
        "                    tf.float32\n",
        "                ) + target_q_values,\n",
        "                axis=1\n",
        "            ) - tf.reduce_sum(masks * target_q_values, axis=1)\n",
        "        )\n",
        "\n",
        "        loss = loss_fn(updated_q_values, q_action) + loss_fn(updated_nstep_q_values, q_action) + lmc_loss + sum(model.losses)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train_step(state):\n",
        "        (state, action, reward, state_next, done, nsteps, nstep_reward_sum, nstep_state_next, nstep_done) = inputs\n",
        "        (target_q_values, updated_q_values, updated_nstep_q_values, masks) = dqfd_metrics(inputs)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = model(state)\n",
        "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "\n",
        "            lmc_loss = tf.reduce_mean(\n",
        "                tf.reduce_max(\n",
        "                    tf.cast(\n",
        "                        tf.equal(\n",
        "                            masks,\n",
        "                            0\n",
        "                        ),\n",
        "                        tf.float32\n",
        "                    ) + target_q_values,\n",
        "                    axis=1\n",
        "                ) - tf.reduce_sum(masks * target_q_values, axis=1)\n",
        "            )\n",
        "\n",
        "            loss = loss_fn(updated_q_values, q_action) + loss_fn(updated_nstep_q_values, q_action) + lmc_loss + sum(model.losses)\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    gamma = 0.99\n",
        "\n",
        "    History = namedtuple('History', 'history')\n",
        "    history = History(history={\n",
        "        'loss': [],\n",
        "        'val_loss': []\n",
        "    })\n",
        "\n",
        "    frame = 0\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss = 0\n",
        "        val_loss = 0\n",
        "\n",
        "        for _ in tqdm(tf.range(steps)):\n",
        "            inputs = next(dataset)\n",
        "            train_loss += train_step(inputs)\n",
        "\n",
        "            frame += 1\n",
        "            if frame % 1000 == 0:\n",
        "                model_target.set_weights(model.get_weights())\n",
        "\n",
        "        for _ in tf.range(val_steps):\n",
        "            inputs = next(val_dataset)\n",
        "            val_loss += val_step(inputs)\n",
        "\n",
        "        history.history['loss'].append(train_loss / steps)\n",
        "        history.history['val_loss'].append(val_loss / val_steps)\n",
        "    \n",
        "    return history\n",
        "\n",
        "register_trainer('dqfd', dqfd)"
      ],
      "metadata": {
        "id": "fxsyDC1tfWiH"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optmized DQFD"
      ],
      "metadata": {
        "id": "9mKmIMSumukh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from collections import namedtuple\n",
        "\n",
        "gamma = 0.99\n",
        "frame = 0\n",
        "\n",
        "def optimized_dqfd(model, model_target, loss_fn, optimizer, dataset, val_dataset, steps, val_steps, epochs):\n",
        "    dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "    val_dataset = strategy.experimental_distribute_dataset(val_dataset)\n",
        "\n",
        "    dataset = iter(dataset)\n",
        "    val_dataset = iter(val_dataset)\n",
        "\n",
        "    def dqfd_metrics(inputs):\n",
        "\n",
        "        (state, action, reward, state_next, done, nsteps, nstep_reward_sum, nstep_state_next, nstep_done) = inputs\n",
        "\n",
        "        target_q_values = model_target(state)\n",
        "        future_rewards = model_target(state_next)\n",
        "        nstep_future_rewards = model_target(nstep_state_next)\n",
        "\n",
        "        max_future_reward =  tf.reduce_max(future_rewards, axis=1)\n",
        "        max_nstep_future_reward = tf.reduce_max(nstep_future_rewards, axis=1)\n",
        "\n",
        "        updated_q_values = reward + gamma * max_future_reward\n",
        "        updated_q_values = updated_q_values * tf.cast(1 - done, dtype=tf.float32) - tf.cast(done, dtype=tf.float32)\n",
        "\n",
        "        updated_nstep_q_values = nstep_reward_sum + gamma ** tf.cast(nsteps, dtype=tf.float32) + max_nstep_future_reward\n",
        "        updated_nstep_q_values = updated_nstep_q_values * tf.cast(1 - nstep_done, dtype=tf.float32) - tf.cast(done, dtype=tf.float32)\n",
        "\n",
        "        masks = tf.one_hot(action, 112)\n",
        "\n",
        "        return (target_q_values, updated_q_values, updated_nstep_q_values, masks)\n",
        "\n",
        "    def train_step(inputs):\n",
        "\n",
        "        (state, action, reward, state_next, done, nsteps, nstep_reward_sum, nstep_state_next, nstep_done) = inputs\n",
        "        (target_q_values, updated_q_values, updated_nstep_q_values, masks) = dqfd_metrics(inputs)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = model(state)\n",
        "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "\n",
        "            lmc_loss = tf.reduce_sum(\n",
        "                tf.reduce_max(\n",
        "                    tf.cast(\n",
        "                        tf.equal(\n",
        "                            masks,\n",
        "                            0\n",
        "                        ),\n",
        "                        tf.float32\n",
        "                    ) + target_q_values,\n",
        "                    axis=1\n",
        "                ) - tf.reduce_sum(masks * target_q_values, axis=1)\n",
        "            ) * (1. / BATCH_SIZE)\n",
        "            \n",
        "            q_loss = loss_fn(updated_q_values, q_action)\n",
        "            nstep_loss = loss_fn(updated_nstep_q_values, q_action)\n",
        "            l2_loss = tf.nn.scale_regularization_loss(model.losses)\n",
        "            loss = q_loss + nstep_loss + lmc_loss + l2_loss\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def val_step(inputs):\n",
        "\n",
        "        (state, action, reward, state_next, done, nsteps, nstep_reward_sum, nstep_state_next, nstep_done) = inputs\n",
        "        (target_q_values, updated_q_values, updated_nstep_q_values, masks) = dqfd_metrics(inputs)\n",
        "\n",
        "        q_values = model(state)\n",
        "        q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "\n",
        "        lmc_loss = tf.reduce_sum(\n",
        "            tf.reduce_max(\n",
        "                tf.cast(\n",
        "                    tf.equal(\n",
        "                        masks,\n",
        "                        0\n",
        "                    ),\n",
        "                    tf.float32\n",
        "                ) + target_q_values,\n",
        "                axis=1\n",
        "            ) - tf.reduce_sum(masks * target_q_values, axis=1)\n",
        "        ) * (1. / BATCH_SIZE)\n",
        "        \n",
        "        q_loss = loss_fn(updated_q_values, q_action)\n",
        "        nstep_loss = loss_fn(updated_nstep_q_values, q_action)\n",
        "        l2_loss = tf.nn.scale_regularization_loss(model.losses)\n",
        "        loss = q_loss + nstep_loss + lmc_loss + l2_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_train_step(inputs):\n",
        "        per_replica_losses = strategy.run(train_step, args=(inputs,))\n",
        "        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_val_step(inputs):\n",
        "        per_replica_losses = strategy.run(val_step, args=(inputs,))\n",
        "        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "    def train_epoch():\n",
        "        global frame\n",
        "        total_loss = 0\n",
        "\n",
        "        for _ in tqdm(tf.range(steps)):\n",
        "            inputs = next(dataset)\n",
        "            # Train step\n",
        "            total_loss += distributed_train_step(inputs)\n",
        "\n",
        "            frame += 1\n",
        "            if frame % 1000 == 0:\n",
        "                model_target.set_weights(model.get_weights())\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def validation():\n",
        "        total_loss = 0\n",
        "        for _ in tf.range(val_steps):\n",
        "            inputs = next(val_dataset)\n",
        "            # Val step\n",
        "            total_loss += distributed_val_step(inputs)\n",
        "        return total_loss\n",
        "\n",
        "    History = namedtuple('History', 'history')\n",
        "    history = History(history={\n",
        "        'loss': [],\n",
        "        'val_loss': []\n",
        "    })\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss = train_epoch()\n",
        "        val_loss = validation()\n",
        "\n",
        "        history.history['loss'].append(train_loss / steps)\n",
        "        history.history['val_loss'].append(val_loss / val_steps)\n",
        "\n",
        "    return history\n",
        "\n",
        "register_trainer('optimized_dqfd', optimized_dqfd)"
      ],
      "metadata": {
        "id": "2IH9E3F2mzTs"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWBZqHjZYGUb"
      },
      "source": [
        "## DQN Epsilon Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "ESpX6wnwYGUc"
      },
      "outputs": [],
      "source": [
        "def train(model, model_target, env):\n",
        "\n",
        "    num_actions = 4\n",
        "\n",
        "    seed = 42\n",
        "    gamma = 0.99\n",
        "    epsilon = 1.0\n",
        "    epsilon_min = 0.1\n",
        "    epsilon_max = 1.0\n",
        "    epsilon_interval = (\n",
        "        epsilon_max - epsilon_min\n",
        "    )\n",
        "    batch_size = 32\n",
        "    max_steps_per_episode = 10000\n",
        "\n",
        "    env.seed(seed)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
        "    loss_function = keras.losses.Huber()\n",
        "\n",
        "    action_history = []\n",
        "    state_history = []\n",
        "    state_next_history = []\n",
        "    rewards_history = []\n",
        "    done_history = []\n",
        "    episode_reward_history = []\n",
        "\n",
        "    frame_sample = []\n",
        "\n",
        "    running_reward = 0\n",
        "    episode_count = 0\n",
        "    frame_count = 0\n",
        "\n",
        "    epsilon_random_frames = 50000\n",
        "    epsilon_greedy_frames = 10000000\n",
        "\n",
        "    max_memory_length = 100000\n",
        "\n",
        "    update_after_actions = 4\n",
        "    update_target_network = 1000\n",
        "\n",
        "    while True:\n",
        "        state = np.array(env.reset())\n",
        "        episode_reward = 0\n",
        "\n",
        "        start = time.time()\n",
        "        for timestep in range(1, max_steps_per_episode):\n",
        "\n",
        "            end = time.time()\n",
        "            frame_sample.append(end - start)\n",
        "            if len(frame_sample) == 60 * 5:\n",
        "                coiso = np.mean(frame_sample)\n",
        "                print(f'FPS: {1 / coiso}')\n",
        "                frame_sample = []\n",
        "            start = time.time()\n",
        "\n",
        "            #env.render()\n",
        "            frame_count += 1\n",
        "\n",
        "            if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
        "                action = np.random.choice(num_actions)\n",
        "            else:\n",
        "                state_tensor = tf.convert_to_tensor(state)\n",
        "                state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "                action_probs = model(state_tensor, training=False)\n",
        "                action = tf.argmax(action_probs[0]).numpy()\n",
        "            \n",
        "            epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "            epsilon = max(epsilon, epsilon_min)\n",
        "\n",
        "            state_next, reward, done, _ = env.step(action)\n",
        "            state_next = np.array(state_next)\n",
        "\n",
        "            episode_reward += reward\n",
        "\n",
        "            action_history.append(action)\n",
        "            state_history.append(state)\n",
        "            state_next_history.append(state_next)\n",
        "            done_history.append(done)\n",
        "            rewards_history.append(reward)\n",
        "            state = state_next\n",
        "\n",
        "            if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "                \n",
        "                indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
        "\n",
        "                state_sample = np.array([state_history[i] for i in indices])\n",
        "                state_next_sample = np.array([state_next_history[i] for i in indices])\n",
        "                rewards_sample = np.array([rewards_history[i] for i in indices])\n",
        "                action_sample = np.array([action_history[i] for i in indices])\n",
        "                done_sample = tf.convert_to_tensor(\n",
        "                    [float(done_history[i]) for i in indices]\n",
        "                )\n",
        "\n",
        "                future_rewards = predict_target(model_target, state_next_sample)\n",
        "                updated_q_values = rewards_sample + gamma * tf.reduce_max (\n",
        "                    future_rewards, axis=1\n",
        "                )\n",
        "                updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
        "\n",
        "                masks = tf.one_hot(action_sample, num_actions)\n",
        "\n",
        "                backpropagation(model, optimizer, loss_function, state_sample, updated_q_values, masks)\n",
        "\n",
        "            if frame_count % update_target_network == 0:\n",
        "                model_target.set_weights(model.get_weights())\n",
        "                template = 'running reward: {:.2f} at episode {}, frame count {}'\n",
        "                print(template.format(running_reward, episode_count, frame_count))\n",
        "\n",
        "            if len(rewards_history) > max_memory_length:\n",
        "                del rewards_history[:1]\n",
        "                del state_history[:1]\n",
        "                del state_next_history[:1]\n",
        "                del action_history[:1]\n",
        "                del done_history[:1]\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            episode_reward_history.append(episode_reward)\n",
        "            if len(episode_reward_history) > 100:\n",
        "                del episode_reward_history[:1]\n",
        "            running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "            episode_count += 1\n",
        "\n",
        "            if running_reward > 40:\n",
        "                print('Solved at episode {}!'.format(episode_count))\n",
        "                break\n",
        "\n",
        "@tf.function\n",
        "def predict_target(model_target, state_next_sample):\n",
        "    future_rewards = model_target(state_next_sample)\n",
        "    return future_rewards\n",
        "\n",
        "@tf.function\n",
        "def backpropagation(model, optimizer, loss_function, state_sample, updated_q_values, masks):\n",
        "    with tf.GradientTape() as tape:\n",
        "        q_values = model(state_sample)\n",
        "        q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "        loss = loss_function(updated_q_values, q_action)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGjH-s9ZYGUU"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {}\n",
        "\n",
        "def register_model(name, model):\n",
        "    models[name] = model"
      ],
      "metadata": {
        "id": "mrjqzbYOc9Q5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTjlYz3CYGUW"
      },
      "source": [
        "## Deepmind Atari"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EagSZZ1jYGUW"
      },
      "outputs": [],
      "source": [
        "def deepmind_atari(input_shape, nb_outputs):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    \n",
        "    x = Conv2D(32, 8, strides=4, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')(inputs)\n",
        "    x = Conv2D(64, 4, strides=4, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')(x)\n",
        "    x = Conv2D(64, 3, strides=1, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(512, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')(x)\n",
        "    output = Dense(nb_outputs, activation='linear', kernel_regularizer='l2', bias_regularizer='l2')(x)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=output)\n",
        "\n",
        "register_model('deepmind_atari', deepmind_atari)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrtTS3OdYGUX"
      },
      "source": [
        "## Xception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "88KAQJ-jYGUY"
      },
      "outputs": [],
      "source": [
        "def xception(input_shape, nb_outputs):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Entry block\n",
        "    x = Rescaling(1.0 / 255)(inputs)\n",
        "    x = Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(64, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    for size in [128, 256, 512, 728]:\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        x = MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = Conv2D(size, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    x = SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    \n",
        "    outputs = Dense(nb_outputs, activation='linear')(x)\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "register_model('xception', xception)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A26gdzdYGUl"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WRAPPER = 'amiranas'\n",
        "MODEL = 'deepmind_atari'\n",
        "TRAINER = 'optimized_dqfd'\n",
        "\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 64 * strategy.num_replicas_in_sync\n",
        "EXAMPLES_PER_FILE = 2048\n",
        "\n",
        "CHECKPOINT = '/content/drive/MyDrive/weights/checkpoint'\n",
        "DATASET_DIR = '/home/minerl/tfrecords'"
      ],
      "metadata": {
        "id": "1rIDGfeUOyxV"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYOyClORkVBM",
        "outputId": "e5e1f7b4-38d8-4ea8-f0e8-9825ff1016d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_expert_data(WRAPPER, EXAMPLES_PER_FILE, DATASET_DIR)"
      ],
      "metadata": {
        "id": "dMeoVc0POveU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcs_pattern = 'gs://minerl_data_records/tfrecords_nstep/*.tfrecord'\n",
        "filenames = tf.io.gfile.glob(gcs_pattern)\n",
        "filenames = filenames[:10]\n",
        "validation_split = 0.1\n",
        "split = len(filenames) - int(len(filenames) * validation_split)\n",
        "train_fns = filenames[:split]\n",
        "validation_fns = filenames[split:]\n",
        "\n",
        "dataset = create_dataset(train_fns, BATCH_SIZE)\n",
        "val_dataset = create_val_dataset(validation_fns, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "M8x0I3mCQ1mc"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    model = models[MODEL]((64, 64, 3), 112)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    model.compile(optimizer, loss_fn, metrics=[val_acc_metric])"
      ],
      "metadata": {
        "id": "J_GyEeck31hx"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[MODEL]((64, 64, 3), 112)\n",
        "model_target = models[MODEL]((64, 64, 3), 112)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.00001)\n",
        "loss_fn = keras.losses.Huber()"
      ],
      "metadata": {
        "id": "qA2UkaYWgcTQ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    model = models[MODEL]((64, 64, 3), 112)\n",
        "    model_target = models[MODEL]((64, 64, 3), 112)\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.00001)\n",
        "    loss_obj = keras.losses.Huber(reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "    def loss_fn(labels, predictions):\n",
        "        per_example_loss = loss_obj(labels, predictions)\n",
        "        return tf.reduce_sum(per_example_loss) * (1. / BATCH_SIZE)"
      ],
      "metadata": {
        "id": "O9-7pkvyqHDC"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(CHECKPOINT)"
      ],
      "metadata": {
        "id": "nGPLAxsatOWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_options = tf.train.CheckpointOptions(experimental_io_device='/job:localhost')\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=CHECKPOINT, save_weights_only=True, save_best_only=True, options=checkpoint_options)"
      ],
      "metadata": {
        "id": "1rNJwmVVjOIx"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = len(train_fns) * EXAMPLES_PER_FILE / BATCH_SIZE\n",
        "validation_steps = len(validation_fns) * EXAMPLES_PER_FILE / BATCH_SIZE\n",
        "\n",
        "history = trainers[TRAINER](model, model_target, loss_fn, optimizer, dataset, val_dataset, steps_per_epoch, validation_steps, EPOCHS)"
      ],
      "metadata": {
        "id": "UtyBsx4igJmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = len(train_fns) * EXAMPLES_PER_FILE / BATCH_SIZE\n",
        "validation_steps = len(validation_fns) * EXAMPLES_PER_FILE / BATCH_SIZE\n",
        "\n",
        "def format_for_classification(state, action, reward, state_next, done, nsteps, nstep_reward_sum, nstep_state_next, nstep_done):\n",
        "    return state, action\n",
        "\n",
        "dataset = dataset.map(format_for_classification)\n",
        "val_dataset = val_dataset.map(format_for_classification)\n",
        "\n",
        "history = model.fit(dataset, validation_data=val_dataset, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=EPOCHS, callbacks=[model_checkpoint_callback])"
      ],
      "metadata": {
        "id": "BzhhDNxs38Mm",
        "outputId": "7643664a-9df1-4655-94a6-52319e349a67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "36/36 [==============================] - 9s 105ms/step - loss: 5.5359 - sparse_categorical_accuracy: 0.5508 - val_loss: 3.4266 - val_sparse_categorical_accuracy: 0.5601\n",
            "Epoch 2/30\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.0521 - sparse_categorical_accuracy: 0.5862 - val_loss: 2.4508 - val_sparse_categorical_accuracy: 0.5601\n",
            "Epoch 3/30\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 2.4634 - sparse_categorical_accuracy: 0.5864 - val_loss: 2.0412 - val_sparse_categorical_accuracy: 0.5601\n",
            "Epoch 4/30\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 2.2234 - sparse_categorical_accuracy: 0.5862 - val_loss: 1.8545 - val_sparse_categorical_accuracy: 0.5601\n",
            "Epoch 5/30\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.1119 - sparse_categorical_accuracy: 0.5862 - val_loss: 1.8409 - val_sparse_categorical_accuracy: 0.5601\n",
            "Epoch 6/30\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.0238 - sparse_categorical_accuracy: 0.5868 - val_loss: 1.7747 - val_sparse_categorical_accuracy: 0.5601\n",
            "Epoch 7/30\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 1.9566 - sparse_categorical_accuracy: 0.5930 - val_loss: 1.7356 - val_sparse_categorical_accuracy: 0.5742\n",
            "Epoch 8/30\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 1.9373 - sparse_categorical_accuracy: 0.6007 - val_loss: 1.6878 - val_sparse_categorical_accuracy: 0.5679\n",
            "Epoch 9/30\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 1.8995 - sparse_categorical_accuracy: 0.5827 - val_loss: 1.6682 - val_sparse_categorical_accuracy: 0.5659\n",
            "Epoch 10/30\n",
            "36/36 [==============================] - 2s 54ms/step - loss: 1.8803 - sparse_categorical_accuracy: 0.5892 - val_loss: 1.6771 - val_sparse_categorical_accuracy: 0.5684\n",
            "Epoch 11/30\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 1.8733 - sparse_categorical_accuracy: 0.5745 - val_loss: 1.6379 - val_sparse_categorical_accuracy: 0.5576\n",
            "Epoch 12/30\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 1.8381 - sparse_categorical_accuracy: 0.5902 - val_loss: 1.5625 - val_sparse_categorical_accuracy: 0.5820\n",
            "Epoch 13/30\n",
            "36/36 [==============================] - 2s 55ms/step - loss: 1.8370 - sparse_categorical_accuracy: 0.5894 - val_loss: 1.6472 - val_sparse_categorical_accuracy: 0.5864\n",
            "Epoch 14/30\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.8018 - sparse_categorical_accuracy: 0.5889 - val_loss: 1.6114 - val_sparse_categorical_accuracy: 0.5815\n",
            "Epoch 15/30\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.7862 - sparse_categorical_accuracy: 0.5864 - val_loss: 1.6179 - val_sparse_categorical_accuracy: 0.5820\n",
            "Epoch 16/30\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 1.7712 - sparse_categorical_accuracy: 0.5934 - val_loss: 1.6238 - val_sparse_categorical_accuracy: 0.5786\n",
            "Epoch 17/30\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.7667 - sparse_categorical_accuracy: 0.5867 - val_loss: 1.5750 - val_sparse_categorical_accuracy: 0.5884\n",
            "Epoch 18/30\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 1.7671 - sparse_categorical_accuracy: 0.5917 - val_loss: 1.5230 - val_sparse_categorical_accuracy: 0.5933\n",
            "Epoch 19/30\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 1.7569 - sparse_categorical_accuracy: 0.5876 - val_loss: 1.6023 - val_sparse_categorical_accuracy: 0.5879\n",
            "Epoch 20/30\n",
            "36/36 [==============================] - 2s 54ms/step - loss: 1.7716 - sparse_categorical_accuracy: 0.5894 - val_loss: 1.6161 - val_sparse_categorical_accuracy: 0.5884\n",
            "Epoch 21/30\n",
            "36/36 [==============================] - 2s 54ms/step - loss: 1.7311 - sparse_categorical_accuracy: 0.5961 - val_loss: 1.6044 - val_sparse_categorical_accuracy: 0.5903\n",
            "Epoch 22/30\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 1.7238 - sparse_categorical_accuracy: 0.5886 - val_loss: 1.5733 - val_sparse_categorical_accuracy: 0.5996\n",
            "Epoch 23/30\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 1.7066 - sparse_categorical_accuracy: 0.5967 - val_loss: 1.5385 - val_sparse_categorical_accuracy: 0.5874\n",
            "Epoch 24/30\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 1.7279 - sparse_categorical_accuracy: 0.5916 - val_loss: 1.5702 - val_sparse_categorical_accuracy: 0.5972\n",
            "Epoch 25/30\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 1.7413 - sparse_categorical_accuracy: 0.5895 - val_loss: 1.6935 - val_sparse_categorical_accuracy: 0.5967\n",
            "Epoch 26/30\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 1.7361 - sparse_categorical_accuracy: 0.5941 - val_loss: 1.5193 - val_sparse_categorical_accuracy: 0.5859\n",
            "Epoch 27/30\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 1.7302 - sparse_categorical_accuracy: 0.5915 - val_loss: 1.5344 - val_sparse_categorical_accuracy: 0.5889\n",
            "Epoch 28/30\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 1.6812 - sparse_categorical_accuracy: 0.5993 - val_loss: 1.5322 - val_sparse_categorical_accuracy: 0.6006\n",
            "Epoch 29/30\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 1.6980 - sparse_categorical_accuracy: 0.5958 - val_loss: 1.5368 - val_sparse_categorical_accuracy: 0.6284\n",
            "Epoch 30/30\n",
            "36/36 [==============================] - 2s 54ms/step - loss: 1.7370 - sparse_categorical_accuracy: 0.5952 - val_loss: 1.5548 - val_sparse_categorical_accuracy: 0.5874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_training_curves(training, validation, title, subplot):\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.plot(training)\n",
        "  ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['training', 'validation'])\n",
        "\n",
        "plt.subplots(figsize=(10,10))\n",
        "plt.tight_layout()\n",
        "#display_training_curves(history.history['sparse_categorical_accuracy'], history.history['val_sparse_categorical_accuracy'], 'accuracy', 211)\n",
        "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
      ],
      "metadata": {
        "id": "mgSV7L5yBDPE",
        "outputId": "5bc9c0c8-2a0c-4b11-e5a9-6cf87d8a00af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jQ0equmGYKbt",
        "-zuJdSSzX9yZ",
        "CtFFwJOuYGUe",
        "YAuJcqFti8zz",
        "gDfAkWunYGUZ",
        "A2piAVS4fTFo",
        "zWBZqHjZYGUb",
        "cGjH-s9ZYGUU",
        "cTjlYz3CYGUW",
        "FrtTS3OdYGUX"
      ],
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}