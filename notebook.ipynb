{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepmind Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense\n",
    "\n",
    "def create_model(input_shape, nb_outputs):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv2D(32, 8, strides=4, activation='relu')(inputs)\n",
    "    x = Conv2D(64, 4, strides=4, activation='relu')(x)\n",
    "    x = Conv2D(64, 3, strides=1, activation='relu')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    output = Dense(nb_outputs, activation='linear')(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Rescaling, BatchNormalization, Activation, SeparableConv2D, Dropout, MaxPooling2D, add\n",
    "\n",
    "def create_model(input_shape, nb_outputs):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Entry block\n",
    "    x = Rescaling(1.0 / 255)(inputs)\n",
    "    x = Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [128, 256, 512, 728]:\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    \n",
    "    outputs = Dense(nb_outputs, activation='linear')(x)\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minerl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treechop Expert Amiranas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minerl\n",
    "import numpy as np\n",
    "from minerl.data import BufferedBatchIter\n",
    "import tensorflow as tf\n",
    "from wrappers.minerl.amiranas import ActionManager\n",
    "\n",
    "def train(model):\n",
    "    manager = ActionManager()\n",
    "\n",
    "    data = minerl.data.make('MineRLTreechop-v0')\n",
    "    iterator = BufferedBatchIter(data, 32000)\n",
    "\n",
    "    def coiso(batch_size=32, num_epochs=1):\n",
    "        for current_state, action, reward, next_state, done in iterator.buffered_batch_iter(batch_size, num_epochs):\n",
    "            x = current_state[\"pov\"].squeeze().astype(np.float32)\n",
    "            x = x / 255\n",
    "            y = manager.get_id(action, batch_size)\n",
    "            yield (x, y)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(optimizer, loss_fn, metrics=[val_acc_metric])\n",
    "\n",
    "    batch_size = 32\n",
    "    num_epochs = 50\n",
    "\n",
    "    model.fit(coiso(batch_size, num_epochs), verbose=1)\n",
    "    model.save_weights('treechop')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treechop Expert Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minerl\n",
    "import numpy as np\n",
    "from minerl.data import BufferedBatchIter\n",
    "import tensorflow as tf\n",
    "from wrappers.minerl.baseline_notebook import dataset_action_batch_to_actions\n",
    "\n",
    "def train(model):\n",
    "    data = minerl.data.make('MineRLTreechop-v0')\n",
    "    iterator = BufferedBatchIter(data, 32000)\n",
    "\n",
    "    def coiso(batch_size=32, num_epochs=1):\n",
    "        for current_state, action, reward, next_state, done in iterator.buffered_batch_iter(batch_size, num_epochs):\n",
    "            x = current_state[\"pov\"].squeeze().astype(np.float32)\n",
    "            y = dataset_action_batch_to_actions(action)\n",
    "            yield (x, y)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(optimizer, loss_fn, metrics=[val_acc_metric])\n",
    "\n",
    "    batch_size = 32\n",
    "    num_epochs = 50\n",
    "\n",
    "    model.fit(coiso(batch_size, num_epochs), verbose=1)\n",
    "    model.save_weights('treechop-fixup')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Epsilon Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import time\n",
    "\n",
    "def train(model, model_target, env):\n",
    "\n",
    "    num_actions = 4\n",
    "\n",
    "    seed = 42\n",
    "    gamma = 0.99\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_max = 1.0\n",
    "    epsilon_interval = (\n",
    "        epsilon_max - epsilon_min\n",
    "    )\n",
    "    batch_size = 32\n",
    "    max_steps_per_episode = 10000\n",
    "\n",
    "    env.seed(seed)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "    loss_function = keras.losses.Huber()\n",
    "\n",
    "    action_history = []\n",
    "    state_history = []\n",
    "    state_next_history = []\n",
    "    rewards_history = []\n",
    "    done_history = []\n",
    "    episode_reward_history = []\n",
    "\n",
    "    frame_sample = []\n",
    "\n",
    "    running_reward = 0\n",
    "    episode_count = 0\n",
    "    frame_count = 0\n",
    "\n",
    "    epsilon_random_frames = 50000\n",
    "    epsilon_greedy_frames = 10000000\n",
    "\n",
    "    max_memory_length = 100000\n",
    "\n",
    "    update_after_actions = 4\n",
    "    update_target_network = 1000\n",
    "\n",
    "    while True:\n",
    "        state = np.array(env.reset())\n",
    "        episode_reward = 0\n",
    "\n",
    "        start = time.time()\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "\n",
    "            end = time.time()\n",
    "            frame_sample.append(end - start)\n",
    "            if len(frame_sample) == 60 * 5:\n",
    "                coiso = np.mean(frame_sample)\n",
    "                print(f'FPS: {1 / coiso}')\n",
    "                frame_sample = []\n",
    "            start = time.time()\n",
    "\n",
    "            #env.render()\n",
    "            frame_count += 1\n",
    "\n",
    "            if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "                action = np.random.choice(num_actions)\n",
    "            else:\n",
    "                state_tensor = tf.convert_to_tensor(state)\n",
    "                state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "                action_probs = model(state_tensor, training=False)\n",
    "                action = tf.argmax(action_probs[0]).numpy()\n",
    "            \n",
    "            epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "            epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "            state_next, reward, done, _ = env.step(action)\n",
    "            state_next = np.array(state_next)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            action_history.append(action)\n",
    "            state_history.append(state)\n",
    "            state_next_history.append(state_next)\n",
    "            done_history.append(done)\n",
    "            rewards_history.append(reward)\n",
    "            state = state_next\n",
    "\n",
    "            if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "                \n",
    "                indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "                state_sample = np.array([state_history[i] for i in indices])\n",
    "                state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "                rewards_sample = np.array([rewards_history[i] for i in indices])\n",
    "                action_sample = np.array([action_history[i] for i in indices])\n",
    "                done_sample = tf.convert_to_tensor(\n",
    "                    [float(done_history[i]) for i in indices]\n",
    "                )\n",
    "\n",
    "                future_rewards = predict_target(model_target, state_next_sample)\n",
    "                updated_q_values = rewards_sample + gamma * tf.reduce_max (\n",
    "                    future_rewards, axis=1\n",
    "                )\n",
    "                updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "                masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "                backpropagation(model, optimizer, loss_function, state_sample, updated_q_values, masks)\n",
    "\n",
    "            if frame_count % update_target_network == 0:\n",
    "                model_target.set_weights(model.get_weights())\n",
    "                template = 'running reward: {:.2f} at episode {}, frame count {}'\n",
    "                print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "            if len(rewards_history) > max_memory_length:\n",
    "                del rewards_history[:1]\n",
    "                del state_history[:1]\n",
    "                del state_next_history[:1]\n",
    "                del action_history[:1]\n",
    "                del done_history[:1]\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            episode_reward_history.append(episode_reward)\n",
    "            if len(episode_reward_history) > 100:\n",
    "                del episode_reward_history[:1]\n",
    "            running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "            episode_count += 1\n",
    "\n",
    "            if running_reward > 40:\n",
    "                print('Solved at episode {}!'.format(episode_count))\n",
    "                break\n",
    "\n",
    "@tf.function\n",
    "def predict_target(model_target, state_next_sample):\n",
    "    future_rewards = model_target(state_next_sample)\n",
    "    return future_rewards\n",
    "\n",
    "@tf.function\n",
    "def backpropagation(model, optimizer, loss_function, state_sample, updated_q_values, masks):\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model(state_sample)\n",
    "        q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "        loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minerl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amiranas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "class ActionManager:\n",
    "    \"\"\"Main minecraft action wrapper. Simplifies action space to 130 discrete actions\"\"\"\n",
    "\n",
    "    def __init__(self, c_action_magnitude=22.5):\n",
    "        self.c_action_magnitude = c_action_magnitude\n",
    "\n",
    "        self.zero_action = OrderedDict([('attack', 0),\n",
    "                                        ('back', 0),\n",
    "                                        ('camera', np.array([0., 0.])),\n",
    "                                        ('forward', 0),\n",
    "                                        ('jump', 0),\n",
    "                                        ('left', 0),\n",
    "                                        ('right', 0),\n",
    "                                        ('sneak', 0),\n",
    "                                        ('sprint', 0)])\n",
    "\n",
    "        # camera discretization:\n",
    "        self.camera_dict = OrderedDict([\n",
    "            ('turn_up', np.array([-c_action_magnitude, 0.])),\n",
    "            ('turn_down', np.array([c_action_magnitude, 0.])),\n",
    "            ('turn_left', np.array([0., -c_action_magnitude])),\n",
    "            ('turn_right', np.array([0., c_action_magnitude]))\n",
    "        ])\n",
    "\n",
    "        self.fully_connected_no_camera = ['attack', 'back', 'forward', 'jump', 'left', 'right', 'sprint']\n",
    "        self.camera_actions = ['turn_up', 'turn_down', 'turn_left', 'turn_right']\n",
    "        self.fully_connected = self.fully_connected_no_camera + self.camera_actions\n",
    "\n",
    "        # following action combinations are excluded:\n",
    "        self.exclude = [('forward', 'back'), ('left', 'right'), ('attack', 'jump'),\n",
    "                        ('turn_up', 'turn_down', 'turn_left', 'turn_right')]\n",
    "\n",
    "        # sprint only allowed when forward is used:\n",
    "        self.only_if = [('sprint', 'forward')]\n",
    "\n",
    "        # Maximal allowed mount of actions within one action:\n",
    "        self.remove_size = 3\n",
    "\n",
    "        # if more than 3 actions are present, actions are removed using this list until only 3 actions remain:\n",
    "        self.remove_first_list = ['sprint', 'left', 'right', 'back',\n",
    "                                  'turn_up', 'turn_down', 'turn_left', 'turn_right',\n",
    "                                  'attack', 'jump', 'forward']\n",
    "\n",
    "        self.fully_connected_list = list(product(range(2), repeat=len(self.fully_connected)))\n",
    "\n",
    "        remove = []\n",
    "        for el in self.fully_connected_list:\n",
    "            for tuple_ in self.exclude:\n",
    "                if sum([el[self.fully_connected.index(a)] for a in tuple_]) > 1:\n",
    "                    if el not in remove:\n",
    "                        remove.append(el)\n",
    "            for a, b in self.only_if:\n",
    "                if el[self.fully_connected.index(a)] == 1 and el[self.fully_connected.index(b)] == 0:\n",
    "                    if el not in remove:\n",
    "                        remove.append(el)\n",
    "            if sum(el) > self.remove_size:\n",
    "                if el not in remove:\n",
    "                    remove.append(el)\n",
    "\n",
    "        for r in remove:\n",
    "            self.fully_connected_list.remove(r)\n",
    "\n",
    "        self.action_list = []\n",
    "        for el in self.fully_connected_list:\n",
    "            new_action = copy.deepcopy(self.zero_action)\n",
    "            for key, value in zip(self.fully_connected, el):\n",
    "                if key in self.camera_actions:\n",
    "                    if value:\n",
    "                        new_action['camera'] = self.camera_dict[key]\n",
    "                else:\n",
    "                    new_action[key] = value\n",
    "            self.action_list.append(new_action)\n",
    "\n",
    "        self.num_action_ids_list = [len(self.action_list)]\n",
    "        self.act_continuous_size = 0\n",
    "\n",
    "    def get_action(self, id_):\n",
    "        a = copy.deepcopy(self.action_list[int(id_)])\n",
    "        a['camera'] += np.random.normal(0., 0.5, 2)\n",
    "        return a\n",
    "\n",
    "    def print_action(self, id_):\n",
    "        a = copy.deepcopy(self.action_list[int(id_)])\n",
    "        out = \"\"\n",
    "        for k, v in a.items():\n",
    "            if k != 'camera':\n",
    "                if v != 0:\n",
    "                    out += f'{k} '\n",
    "            else:\n",
    "                if (v != np.zeros(2)).any():\n",
    "                    out += k\n",
    "\n",
    "        print(out)\n",
    "\n",
    "    def get_id(self, action, batch_size):\n",
    "\n",
    "        coiso = np.zeros((batch_size,), dtype=int)\n",
    "        for i in range(batch_size):\n",
    "            action = copy.deepcopy(action)\n",
    "\n",
    "            # discretize 'camera':\n",
    "            camera = action['camera'][i]\n",
    "            camera_action_amount = 0\n",
    "            if - self.c_action_magnitude / 2. < camera[0] < self.c_action_magnitude / 2.:\n",
    "                action['camera'][i][0] = 0.\n",
    "                if - self.c_action_magnitude / 2. < camera[1] < self.c_action_magnitude / 2.:\n",
    "                    action['camera'][i][1] = 0.\n",
    "                else:\n",
    "                    camera_action_amount = 1\n",
    "                    action['camera'][i][1] = self.c_action_magnitude * np.sign(camera[1])\n",
    "            else:\n",
    "                camera_action_amount = 1\n",
    "                action['camera'][i][0] = self.c_action_magnitude * np.sign(camera[0])\n",
    "\n",
    "                action['camera'][i][1] = 0.\n",
    "\n",
    "            # simplify action:\n",
    "            for tuple_ in self.exclude:\n",
    "                if len(tuple_) == 2:\n",
    "                    a, b = tuple_\n",
    "                    if action[a][i] and action[b][i]:\n",
    "                        action[b][i] = 0\n",
    "            for a, b in self.only_if:\n",
    "                if not action[b][i]:\n",
    "                    if action[a][i]:\n",
    "                        action[a][i] = 0\n",
    "            for a in self.remove_first_list:\n",
    "                if sum([action[key][i] for key in self.fully_connected_no_camera]) > \\\n",
    "                        (self.remove_size - camera_action_amount):\n",
    "                    if a in self.camera_actions:\n",
    "                        action['camera'][i] = np.array([0., 0.])\n",
    "                        camera_action_amount = 0\n",
    "                    else:\n",
    "                        action[a][i] = 0\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # set one_hot camera keys:\n",
    "            for key in self.camera_actions:\n",
    "                action[key] = [0 for x in range(batch_size)]\n",
    "            for key, val in self.camera_dict.items():\n",
    "                if (action['camera'][i] == val).all():\n",
    "                    action[key][i] = 1\n",
    "                    break\n",
    "\n",
    "            non_separate_values = tuple(action[key][i] for key in self.fully_connected)\n",
    "\n",
    "            coiso[i] = self.fully_connected_list.index(non_separate_values)\n",
    "        return coiso\n",
    "\n",
    "    def get_left_right_reversed_mapping(self):\n",
    "        action_mapping = []\n",
    "        for action in self.action_list:\n",
    "            reversed_action = copy.deepcopy(action)\n",
    "            if action['left'] == 1:\n",
    "                reversed_action['left'] = 0\n",
    "                reversed_action['right'] = 1\n",
    "                assert action['right'] == 0\n",
    "            if action['right'] == 1:\n",
    "                reversed_action['right'] = 0\n",
    "                reversed_action['left'] = 1\n",
    "                assert action['left'] == 0\n",
    "            if (action['camera'] == [0, -22.5]).all():\n",
    "                reversed_action['camera'][1] = 22.5\n",
    "            if (action['camera'] == [0, 22.5]).all():\n",
    "                reversed_action['camera'][1] = -22.5\n",
    "\n",
    "            rev_action_id = self.get_id(reversed_action)\n",
    "            action_mapping.append(rev_action_id)\n",
    "\n",
    "        return action_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dataset_action_batch_to_actions(dataset_actions, camera_margin=3):\n",
    "    \"\"\"\n",
    "    Turn a batch of actions from dataset (`batch_iter`) to a numpy\n",
    "    array that corresponds to batch of actions of ActionShaping wrapper (_actions).\n",
    "\n",
    "    Camera margin sets the threshold what is considered \"moving camera\".\n",
    "\n",
    "    Note: Hardcoded to work for actions in ActionShaping._actions, with \"intuitive\"\n",
    "        ordering of actions.\n",
    "        If you change ActionShaping._actions, remember to change this!\n",
    "\n",
    "    Array elements are integers corresponding to actions, or \"-1\"\n",
    "    for actions that did not have any corresponding discrete match.\n",
    "    \"\"\"\n",
    "    # There are dummy dimensions of shape one\n",
    "    camera_actions = dataset_actions[\"camera\"].squeeze()\n",
    "    attack_actions = dataset_actions[\"attack\"].squeeze()\n",
    "    forward_actions = dataset_actions[\"forward\"].squeeze()\n",
    "    jump_actions = dataset_actions[\"jump\"].squeeze()\n",
    "    batch_size = len(camera_actions)\n",
    "    actions = np.zeros((batch_size,), dtype=int)\n",
    "\n",
    "    for i in range(len(camera_actions)):\n",
    "        # Moving camera is most important (horizontal first)\n",
    "        if camera_actions[i][0] < -camera_margin:\n",
    "            actions[i] = 4\n",
    "        elif camera_actions[i][0] > camera_margin:\n",
    "            actions[i] = 5\n",
    "        elif camera_actions[i][1] > camera_margin:\n",
    "            actions[i] = 6\n",
    "        elif camera_actions[i][1] < -camera_margin:\n",
    "            actions[i] = 7\n",
    "        elif forward_actions[i] == 1:\n",
    "            if jump_actions[i] == 1:\n",
    "                actions[i] = 3\n",
    "            else:\n",
    "                actions[i] = 2\n",
    "        elif attack_actions[i] == 1:\n",
    "            actions[i] = 1\n",
    "        else:\n",
    "            # No reasonable mapping (would be no-op)\n",
    "            actions[i] = 0\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.xception import create_model\n",
    "from trainers.minerl.treechop_expert_amiranas import train\n",
    "\n",
    "model = create_model((64, 64, 3), 112)\n",
    "\n",
    "train(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
