# Objetivo

Analisar, reproduzir e possivelmente incrementar soluções existentes de Reinforcement Learning para os desafios propostos na competição [MineRL](https://minerl.io/).

# Refêrencias

1. [MineRL](https://minerl.io/)
2. [MineRL Docs](https://minerl.readthedocs.io/en/latest/index.html)
3. [MineRL 2021](https://www.aicrowd.com/challenges/neurips-2021-minerl-diamond-competition)
4. [The MineRL competition](https://slideslive.at/38922880/the-minerl-competition?ref=search)
5. [Behavioural cloning baseline for the Intro track](https://www.aicrowd.com/showcase/behavioural-cloning-baseline-for-the-intro-track)
6. [Keras](https://keras.io/)
7. [Deep Q-Learning for Atari Breakout](https://keras.io/examples/rl/deep_q_network_breakout/)
8. [Image classification from scratch](https://keras.io/examples/vision/image_classification_from_scratch/)
9. [KerasRL](https://github.com/keras-rl/keras-rl)
10. [Deep Q-learning from Demonstrations](https://arxiv.org/abs/1704.03732)
11. [Boosted Bellman Residual Minimization Handling Expert Demonstrations](https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=63743AEB744A711AB50F1B0F395EE4C2?doi=10.1.1.653.2907&rep=rep1&type=pdf)
12. [Meta Learning Shared Hierarchies](https://arxiv.org/abs/1710.09767)
13. [Discriminator Soft Actor Critic without Extrinsic Rewards](https://arxiv.org/abs/2001.06808)
14. [minerl_imitation_learning](https://github.com/amiranas/minerl_imitation_learning)
